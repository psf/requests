name: Test Coverage Validation

on:
  pull_request:
    branches: [main, master]
    types: [opened, synchronize, reopened, ready_for_review]

jobs:
  coverage-validation:
    runs-on: ubuntu-latest
    if: github.event.pull_request.draft == false
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install requests dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e .
        pip install pytest pytest-cov

    - name: Install cdlreq for coverage analysis
      run: |
        # Clone and install cdlreq (assuming it's available on GitHub)
        # Adjust this step based on where cdlreq is hosted
        git clone https://github.com/SeanHgh/cdlreq.git /tmp/cdlreq || echo "cdlreq clone failed, using alternative method"
        
        # Try to install from parent directory if available
        if [ -d "../cdlreq" ]; then
          echo "Using local cdlreq from parent directory"
          pip install -e ../cdlreq
        elif [ -d "/tmp/cdlreq" ]; then
          echo "Using cloned cdlreq"
          pip install -e /tmp/cdlreq
        else
          echo "Installing cdlreq from PyPI or falling back"
          pip install pyyaml jsonschema click openpyxl  # cdlreq dependencies
          # For now, we'll copy the cdlreq code inline if needed
        fi

    - name: Run pytest and capture output
      run: |
        echo "üß™ Running pytest and capturing output for coverage analysis..."
        python -m pytest tests/ -v --tb=short > test_output.txt 2>&1 || true
        echo "üìã Test output captured to test_output.txt"
        echo ""
        echo "Test output preview (first 30 lines):"
        head -30 test_output.txt
        echo ""
        echo "Test output preview (last 10 lines):"
        tail -10 test_output.txt

    - name: Validate test coverage using cdlreq
      id: coverage_check
      run: |
        echo "üîç Running cdlreq coverage analysis..."
        
        # Try different ways to run cdlreq coverage
        COVERAGE_EXIT_CODE=0
        COVERAGE_OUTPUT=""
        
        # Method 1: Try installed cdlreq
        if command -v cdlreq &> /dev/null; then
          echo "Using installed cdlreq command"
          COVERAGE_OUTPUT=$(cdlreq coverage test_output.txt --directory . 2>&1) || COVERAGE_EXIT_CODE=$?
        # Method 2: Try Python module
        elif python -c "import cdlreq" 2>/dev/null; then
          echo "Using cdlreq Python module"
          COVERAGE_OUTPUT=$(python -c "from cdlreq.cli.commands import cli; cli()" coverage test_output.txt --directory . 2>&1) || COVERAGE_EXIT_CODE=$?
        # Method 3: Use inline implementation as fallback
        else
          echo "Using fallback coverage analysis"
          
          # Simple Python script to analyze coverage
          python3 << 'EOF' > coverage_analysis.py
import yaml
import os
from pathlib import Path

def analyze_coverage():
    print("üìä Analyzing test coverage...")
    
    # Find all specification files
    specs_dir = Path("requirements/specifications")
    if not specs_dir.exists():
        print("‚ùå No specifications directory found")
        return 1
    
    # Read test output
    try:
        with open("test_output.txt", "r") as f:
            test_output = f.read()
    except FileNotFoundError:
        print("‚ùå Test output file not found")
        return 1
    
    # Analyze specifications
    invalid_files = {}
    executed = set()
    not_executed = set()
    
    for spec_file in specs_dir.glob("*.yaml"):
        try:
            with open(spec_file, "r") as f:
                spec = yaml.safe_load(f)
            
            if "unit_test" in spec and spec["unit_test"]:
                test_path = spec["unit_test"]
                spec_id = spec.get("id", "unknown")
                
                # Check if test file exists
                if Path(test_path).exists():
                    # Check if test was executed
                    if test_path in test_output:
                        executed.add((test_path, spec_id))
                    else:
                        not_executed.add((test_path, spec_id))
                else:
                    if test_path not in invalid_files:
                        invalid_files[test_path] = []
                    invalid_files[test_path].append(spec_id)
        
        except Exception as e:
            print(f"‚ö†Ô∏è  Error processing {spec_file}: {e}")
    
    # Display results
    if invalid_files:
        print("‚ö†Ô∏è  Invalid test files (do not exist):")
        for test_path, spec_ids in invalid_files.items():
            spec_list = ", ".join(spec_ids)
            print(f"  {test_path} ‚Üí used in: {spec_list}")
        print()
    
    if executed:
        print("‚úÖ Executed tests:")
        for test_path, spec_id in sorted(executed):
            print(f"  {test_path} ‚Üí used in: {spec_id}")
        print()
    
    if not_executed:
        print("‚ùå Not executed:")
        for test_path, spec_id in sorted(not_executed):
            print(f"  {test_path} ‚Üí used in: {spec_id}")
        print()
    
    # Summary
    valid_tests = len(executed) + len(not_executed)
    print(f"{len(executed)}/{valid_tests} valid tests executed")
    if invalid_files:
        print(f"{len(invalid_files)} invalid test file(s) in specifications")
    
    # Return exit code
    if not_executed or invalid_files:
        return 1
    return 0

if __name__ == "__main__":
    import sys
    sys.exit(analyze_coverage())
EOF
          
          COVERAGE_OUTPUT=$(python3 coverage_analysis.py 2>&1) || COVERAGE_EXIT_CODE=$?
        fi
        
        echo "Coverage analysis output:"
        echo "$COVERAGE_OUTPUT"
        echo ""
        
        # Check results
        if echo "$COVERAGE_OUTPUT" | grep -q "‚ùå Not executed:" || echo "$COVERAGE_OUTPUT" | grep -q "‚ö†Ô∏è  Invalid test files"; then
          echo "‚ùå COVERAGE VALIDATION FAILED"
          echo "Some specification unit test files are not executed or invalid"
          echo "coverage_passed=false" >> $GITHUB_OUTPUT
          echo "coverage_output<<EOF" >> $GITHUB_OUTPUT
          echo "$COVERAGE_OUTPUT" >> $GITHUB_OUTPUT  
          echo "EOF" >> $GITHUB_OUTPUT
          exit 1
        else
          echo "‚úÖ COVERAGE VALIDATION PASSED"
          echo "All specification unit test files are properly covered"
          echo "coverage_passed=true" >> $GITHUB_OUTPUT
          echo "coverage_output<<EOF" >> $GITHUB_OUTPUT
          echo "$COVERAGE_OUTPUT" >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT
          exit 0
        fi

    - name: Comment on PR with coverage results
      if: failure() || success()
      uses: actions/github-script@v7
      with:
        github-token: ${{ secrets.GITHUB_TOKEN }}
        script: |
          const fs = require('fs');
          const coveragePassed = '${{ steps.coverage_check.outputs.coverage_passed }}' === 'true';
          const coverageOutput = `${{ steps.coverage_check.outputs.coverage_output }}`;
          
          const icon = coveragePassed ? '‚úÖ' : '‚ùå';
          const status = coveragePassed ? 'PASSED' : 'FAILED';
          const color = coveragePassed ? 'üü¢' : 'üî¥';
          
          let body = `## ${icon} Test Coverage Validation ${status}\n\n`;
          
          if (coveragePassed) {
            body += `${color} **All specification unit tests are properly covered!**\n\n`;
            body += `All unit test files referenced in your specifications were executed during pytest testing.\n\n`;
          } else {
            body += `${color} **Test coverage validation failed!**\n\n`;
            body += `‚ùå **This pull request cannot be merged** until all issues are resolved.\n\n`;
            body += `Some unit test files referenced in specifications were either:\n`;
            body += `- Not executed during pytest\n`;
            body += `- Invalid/non-existent file paths\n\n`;
          }
          
          body += `### üìä Coverage Analysis Results\n\n`;
          body += '```\n' + coverageOutput + '\n```\n\n';
          
          if (!coveragePassed) {
            body += `### üîß How to Fix\n\n`;
            body += `**For "Not executed" tests:**\n`;
            body += `1. Make sure the test files are discoverable by pytest\n`;
            body += `2. Check that test functions are properly named (start with \`test_\`)\n`;
            body += `3. Ensure test files don't have syntax errors or import issues\n`;
            body += `4. Verify the test files are in the correct directory structure\n\n`;
            body += `**For "Invalid test files":**\n`;
            body += `1. Create the missing test files referenced in specifications\n`;
            body += `2. Update specification \`unit_test\` paths to point to existing files\n`;
            body += `3. Use correct relative paths from the repository root\n\n`;
            body += `**Testing locally:**\n`;
            body += `\`\`\`bash\n`;
            body += `# Run pytest and check coverage\n`;
            body += `python -m pytest tests/ -v > test_output.txt 2>&1\n`;
            body += `# Analyze with cdlreq (if available)\n`;
            body += `cdlreq coverage test_output.txt --directory .\n`;
            body += `\`\`\`\n\n`;
          }
          
          body += `---\n*This check ensures that all unit test files referenced in specifications are actually executed during testing.*`;
          
          // Find existing coverage comment and update it, or create new one
          const { data: comments } = await github.rest.issues.listComments({
            owner: context.repo.owner,
            repo: context.repo.repo,
            issue_number: context.issue.number,
          });
          
          const existingComment = comments.find(comment => 
            comment.user.type === 'Bot' && comment.body.includes('Test Coverage Validation')
          );
          
          if (existingComment) {
            await github.rest.issues.updateComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              comment_id: existingComment.id,
              body: body
            });
          } else {
            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
              body: body
            });
          }

    - name: Upload test artifacts
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: test-coverage-artifacts
        path: |
          test_output.txt
          coverage_analysis.py
        retention-days: 7

  # Summary job for branch protection  
  coverage-summary:
    runs-on: ubuntu-latest
    needs: [coverage-validation]
    if: always()
    
    steps:
    - name: Check coverage validation status
      run: |
        if [[ "${{ needs.coverage-validation.result }}" == "success" ]]; then
          echo "‚úÖ Test coverage validation passed!"
          echo "All specification unit test files are properly covered."
          exit 0
        else
          echo "‚ùå Test coverage validation failed!"
          echo "Some specification unit test files are not executed or invalid."
          echo "This pull request cannot be merged until all issues are resolved."
          exit 1
        fi